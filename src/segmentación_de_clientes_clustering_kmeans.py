# -*- coding: utf-8 -*-
"""Final Asociación y Segmentación De Clientes y Productos Mediante la Identificación de Patrones De Consumo Aplicando clustering y El  Market Baske Analisys (MBA) a compras minoristas online observadas mayoritariamente en Reino Unido entre Dic2010 y Dic20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m9feqxSTgEr0ebPFzv7LBjK3HrPyCFh4

### **Asociación y Segmentación De Clientes y Productos Mediante la Identificación de Patrones De Consumo Aplicando clustering y El  Market Basket Analisys (MBA) a compras minoristas online observadas mayoritariamente en Reino Unido entre el 01 de diciembre de 2010 y el 09 de diciembre de 2011.**

### Estructura del informe
1. Introduccion
2. Conceptos
3. Hipotesis
4. Preprocesamiento
5. EDA
6. Desarrollo de modelos de machine learning
7. Comparacion de modelos
8. Visualizacion del modelo
9. Conclusiones y recomendaciones
10. Referencias

### 1.Introducción:

La dinámica actúal de la vida ha hecho que el mercado entre en nuestras casas a través de la conexion de internet, derivado del comercio eléctronico, también conocido como e-commerce, y se refiere a la compra y venta de productos y servicisos haciendo uso de la conexión web. Esto implica la interacción directa entre consumidores y vendedores, lo que desemboca en la transferencia no solo del dinero para pagar las compras, sino además de los datos de cada consumidor o potencial consumidor que interactuó en el proceso de compra online.

Los datos que intercambian los consumidores a la hora de comprar se vuelven de vital importancia porque con ellos se pueden establecer patrones de comportamientos a la hora de comprar por parte de dichos consumidores. Estos patrones van desde el volúmen de compra y la cantidad de dinero que gastan, hasta el conjunto de productos que habitualmente adquiren juntos.

A continuación se desarrollan modelos de Machine Learning para por un lado, la segmentación de clientes utilizando el algoritmo de clustering, y por el otro lado se desrrollará un modelo de Market Basket Analisys (MBA) para determinar aquellos productos que habitualmente se consumen juntos.

Primeramente se definen los conceptos básicos para que el lector comprenda el tema tratado. Seguidamente se plantea la hiótesis a demostrar, posteriormente se realizarán todos las adecuaciones a la base de datos conocido como el preprocesamiento para internarnos en el Análisis Exploratorio de los Datos (EDA por sus siglas en inglés) a continuación se desarrollan los modelos de machine learning mencionados anteriormente, se muestran los visualmente los resultados obtenidos  y se mencionan las conclusiones alcanzadas y las recomendciones.

Los datos que intercambian los consumidores a la hora de comprar se vuelven de vital importancia porque con ellos se pueden establecer patrones de comportamientos a la hora de comprar por parte de dichos consumidores. Estos patrones van desde el volúmen de compra y la cantidad de dinero que gastan, hasta el conjunto de productos que habitualmente adquiren juntos.

A continuación se desarrollan modelos de Machine Learning para por un lado, la segmentación de clientes utilizando el algoritmo de clustering, y por el otro lado se desrrollará un modelo de Market Basket Analisys (MBA) para determinar aquellos productos que habitualmente se consumen juntos.

Primeramente se definen los conceptos básicos para que el lector comprenda el tema tratado. Seguidamente se plantea la hiótesis a demostrar, posteriormente se realizarán todos las adecuaciones a la base de datos conocido como el preprocesamiento para internarnos en el Análisis Exploratorio de los Datos (EDA por sus siglas en inglés) a continuación se desarrollan los modelos de machine learning mencionados anteriormente, se muestran los visualmente los resultados obtenidos  y se mencionan las conclusiones alcanzadas y las recomendciones.

### 2.Conceptos Básicos:

### ¿Qué es el comercio electrónico?

El comercio electrónico, también conocido como e-commerce, es la compraventa de bienes y servicios a través de Internet y otros medios digitales. Este modelo permite a individuos y empresas realizar transacciones comerciales sin la necesidad de contacto físico, utilizando plataformas como sitios web, aplicaciones móviles y redes sociales.

### Características principales

Permite la compra y venta de productos y servicios en cualquier momento y desde cualquier lugar con acceso a Internet.

Utiliza plataformas digitales como tiendas virtuales, marketplaces, redes sociales y aplicaciones móviles.

Facilita la interacción entre diferentes tipos de participantes: empresas, consumidores, gobiernos y otros negocios.

Incluye tanto bienes físicos como digitales, así como servicios de todo tipo.

Opera las 24 horas del día, ampliando el alcance y la disponibilidad de los productos.

### Tipos de comercio electrónico

1. B2C	Business to Consumer:Empresas venden directamente al consumidor final

2. B2B	Business to Business: Transacciones entre empresas.

3. C2C	Consumer to Consumer: Intercambio entre consumidores, usualmente en plataformas de terceros.

4. B2G	Business to Government: Empresas venden productos o servicios a gobiernos.

### Ventajas del comercio electrónico

Acceso a mercados globales y mayor alcance de clientes potenciales.

Reducción de costos operativos y logísticos en comparación con el comercio tradicional.

Facilidad para comparar precios, productos y servicios en tiempo real.

Disponibilidad permanente, sin limitaciones de horario.

Personalización de la experiencia de compra y estrategias de marketing digital

Algunas de las plataformas más conocidas de comercio electrónico incluyen Amazon, eBay, Mercado Libre y Alibaba. Además, muchas empresas gestionan sus propias tiendas virtuales o utilizan redes sociales para vender productos y servicios

Evolución y tendencias

El comercio electrónico ha experimentado un crecimiento acelerado en los últimos años, impulsado por la digitalización, la expansión del acceso a Internet y cambios en los hábitos de consumo. Las ventas en línea continúan aumentando y se espera que esta tendencia se mantenga en el futuro.

En resumen, el comercio electrónico es una modalidad de intercambio comercial que aprovecha las tecnologías digitales para facilitar la compraventa de bienes y servicios, transformando la manera en que empresas y consumidores interactúan en el mercado global.

### Clustering

El clustering es un algoritmo de aprendizaje automático no supervisado cuyo objetivo principal es organizar y clasificar un conjunto de datos en grupos o clústeres basándose en la similitud inherente entre los objetos o puntos de datos. En esencia, busca agrupaciones naturales dentro de los datos para identificar categorías y las características que las definen.

El clustering se utiliza en diversas etapas del análisis de datos, desde la exploración inicial para descubrir tendencias y valores atípicos, hasta el preprocesamiento para dividir o reducir la dimensionalidad de grandes conjuntos de datos. Los resultados del clustering pueden ser géneros musicales, grupos de usuarios, segmentos de mercado, entre otros, utilizando una o múltiples características de los datos.

El proceso de clustering puede ayudar a encontrar relaciones subyacentes, detectar anomalías (identificando puntos fuera de los clústeres), reducir la complejidad de los datos mediante la reducción de dimensionalidad, y facilitar la visualización de los conjuntos de datos.

Los algoritmos de clustering se distinguen por realizar un clustering duro (cada punto pertenece a un único clúster) o un clustering blando (cada punto tiene una probabilidad de pertenecer a cada clúster). La elección del algoritmo depende de las necesidades específicas del proyecto y las características de los datos.

###Tipos de Clustering

Existen diversos enfoques para el clustering, cada uno con sus propias metodologías para definir y formar los clústeres. A continuación, se resumen los tipos principales mencionados:

- Clustering basado en centroides: Agrupa los datos en función de la distancia a los centroides (media o mediana de los puntos de cada clúster). El algoritmo de K-Means es el ejemplo más común, buscando minimizar la distancia total entre cada punto y su centroide asignado. K-Medoids es una variante que utiliza puntos de datos reales como centroides (medoides), siendo menos sensible a los valores atípicos.

- Clustering jerárquico: Construye una jerarquía de clústeres basada en la proximidad y conectividad de los puntos de datos. No requiere especificar el número de clústeres previamente y puede representarse mediante un dendrograma. Existen dos enfoques:

  1. Aglomerativo (ascendente): Comienza con cada punto como un clúster individual y los fusiona iterativamente según su similitud.
  2. Divisivo (descendente): Comienza con un único clúster que contiene todos los puntos y lo divide sucesivamente.
- Clustering basado en distribución: Asume que los datos están generados por una mezcla de distribuciones de probabilidad (a menudo gaussianas). El algoritmo busca ajustar estas distribuciones para identificar los clústeres. Los Modelos de Mezcla Gaussiana (GMM) utilizando la maximización de expectativas son un ejemplo común, permitiendo una asignación blanda de los puntos a los clústeres.

- Clustering basado en densidad: Identifica clústeres como áreas densas de puntos separadas por regiones de baja densidad. Puede descubrir clústeres de formas arbitrarias y es robusto al ruido. DBSCAN y su variante HDBSCAN son algoritmos populares en este enfoque.

- Clustering basado en cuadrícula: Divide el espacio de datos en una cuadrícula de celdas y agrupa las celdas (y los puntos dentro de ellas) según ciertas características. Es eficiente para datos de alta dimensión. STING y CLIQUE son ejemplos de algoritmos basados en cuadrículas.

### Evaluación del Clustering
La evaluación de los resultados del clustering es crucial para determinar la calidad de las agrupaciones obtenidas. Las métricas de evaluación se dividen en dos categorías:

1. Medidas intrínsecas: Evalúan la calidad del clustering utilizando solo la información del conjunto de datos, sin necesidad de etiquetas externas. Ejemplos incluyen la puntuación de silueta, el índice de Davies-Bouldin y el índice de Calinski-Harabasz.

2. Medidas extrínsecas: Requieren información externa o etiquetas reales para comparar la estructura de los clústeres descubiertos con la verdad fundamental. Ejemplos incluyen la puntuación F, la pureza, el índice Rand y la variación de la información.

### Aplicaciones del Clustering
El clustering tiene una amplia gama de aplicaciones en diversos campos, incluyendo:

- Detección de anomalías: Identificación de puntos de datos que no encajan en ningún clúster bien definido.
Investigación de mercado: Segmentación de clientes basada en características y comportamientos.
- Segmentación de imágenes: Agrupación de píxeles para identificar objetos o regiones.
- Procesamiento de documentos: Agrupación de documentos por similitud de contenido.

### Market Basket Analisys (MBA)

El análisis de la cesta de la compra, también conocido como análisis de afinidad, es una técnica de modelado que se basa en la idea de que la compra de un conjunto específico de productos aumenta la probabilidad de adquirir otros adicionales. Un ejemplo clásico es la alta probabilidad de que un comprador de mantequilla de maní y pan también compre mermelada. Aunque algunas de estas relaciones son intuitivas, el análisis de la cesta de la compra permite descubrir conexiones menos obvias, lo que puede mejorar la predicción del comportamiento del consumidor, impulsar las ventas y ofrecer una ventaja competitiva a los comerciantes.

Es importante aclarar que el análisis de la cesta de la compra es una aplicación específica de las técnicas de análisis de asociación, aunque a menudo estos términos se utilizan indistintamente. Desde la perspectiva del aprendizaje automático, esta técnica se clasifica como aprendizaje no supervisado. Las conclusiones obtenidas a través de este análisis pueden ser posteriormente exploradas con otras herramientas de inteligencia artificial o ciencia de datos para obtener una comprensión más profunda.

A pesar de su capacidad para revelar patrones ocultos en los datos de compra, el análisis de la cesta de la compra es conceptualmente fácil de entender y no requiere un conocimiento avanzado de cálculo o estadística. Sin embargo, es fundamental comprender algunos términos y notaciones clave.

Se utilizan los conceptos de antecedente (la causa o el conjunto de artículos comprados inicialmente) y consecuente (el efecto o el artículo que es probable que se compre a continuación). En el ejemplo anterior, "{Mantequilla de Cacahuete, Pan}" sería el antecedente, y "{Mermelada}" el consecuente, lo que se representa formalmente como {Mantequilla de Cacahuete, Pan} -> {Mermelada}. Es importante destacar que tanto los antecedentes como los consecuentes pueden estar compuestos por múltiples artículos.

Para llevar a cabo el análisis de la cesta de la compra, se consideran tres medidas matemáticas fundamentales: soporte, confianza y mejora.

El soporte indica la frecuencia con la que un conjunto de artículos (generalmente el antecedente y el consecuente juntos) aparece en las transacciones. Por ejemplo, si en 100 transacciones, la "Mantequilla de Cacahuete" y la "Mermelada de Uva" se compran juntas 11 veces, el soporte sería del 11% o 0.11. La confianza se calcula dividiendo el soporte (la probabilidad de que ambos artículos se compren juntos) por la probabilidad de que solo se compre el consecuente. En el ejemplo, si la probabilidad de comprar "Mermelada de Uva" sola es de, digamos, 0.13, la confianza de {Mantequilla de Cacahuete} -> {Mermelada de Uva} sería 0.11 / 0.13 ≈ 0.846, lo que significa que aproximadamente el 85% de las veces que se compró mermelada de uva, también se compró mantequilla de cacahuete.

Finalmente, la mejora mide el aumento en la probabilidad de comprar el consecuente dado que se ha comprado el antecedente, en comparación con la probabilidad de comprar el consecuente independientemente del antecedente. Se calcula dividiendo la confianza por la probabilidad de que se compre el antecedente. Si la probabilidad de comprar "Mantequilla de Cacahuete" es de 0.15, la mejora sería 0.846 / 0.15 ≈ 5.64.

La aplicación de los modelos anteriormente descritos se hará a un data set que contiene las observaciones de compra mayoritariamente en el Reino Unido entre el mes de diciembre de 2012 y diciembre de 2011

## 3. HIPÓTESIS:
#### Entre diciembre de 2010 y diciembre de 2011 en el Reino Unido, los patrones de consumo pueden ser segmentados mediante algoritmos de clustering, lo que permitirá identificar grupos homogéneos de clientes basados en sus hábitos de compra. Al aplicar el  MarketBasket Analysis (MBA) a estos segmentos, se podrán establecer asociaciones significativas entre productos que son frecuentemente adquiridos juntos, optimizando así las estrategias de marketing y mejorando la comprensión de las preferencias de los consumidores.

## 4.Preprocesamiento:

### **4.1. Carga de datos. Objetivo: Obtener los datos de la fuente y asi poder obtener una primera visión de sus propiedades y presentación.
"""

# Importación de las librerias pertinentes para llevar a cabo los análisis

import pickle
#import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from datetime import datetime
from sklearn.model_selection import ( train_test_split, GridSearchCV )
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import ( MinMaxScaler, LabelEncoder )
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    matthews_corrcoef,
    classification_report,
    ConfusionMatrixDisplay )
from sklearn.inspection import permutation_importance
#logger = logging.getLogger() logger.setLevel(logging.ERROR)

from sklearn.feature_extraction.text import TfidfVectorizer
import re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from collections import Counter
from nltk.tag import pos_tag
import os
import time
from nltk.tokenize import word_tokenize

df_raw = pd.read_csv('data/raw/online_retail.csv')
df_raw.info()

num_rows, num_cols = df_raw.shape
print(f"El data set tiene {num_rows} filas y {num_cols} columnas.")

"""Como podemos observar el data set consta de 9 columnas"""

df_raw.sample(10)

# Crear grafico de Distribución de Países en el Conjunto de Datos

distribucion_porcentual = df_raw['Country'].value_counts(normalize=True) * 100
valores = df_raw['Country'].value_counts()

# Crear una sola figura y un solo eje
fig, ax1 = plt.subplots(figsize=(12, 6))

# Graficar valores en el eje y izquierdo (barras)
bars = ax1.bar(valores.index, valores.values, color='blue', alpha=0.7)
ax1.set_xlabel('País')
ax1.set_ylabel('Cantidad', color='blue')
ax1.tick_params(axis='y', labelcolor='blue')

# Establecer las ticks en el eje x
ax1.set_xticks(range(len(valores.index)))
# Establecer las etiquetas correspondientes y rotarlas
ax1.set_xticklabels(valores.index, rotation=45, ha='right')
fig.tight_layout()

# Crear un segundo eje y para graficar los porcentajes (línea)
ax2 = ax1.twinx()
line = ax2.plot(distribucion_porcentual.index, distribucion_porcentual.values, color='salmon', marker='o', linestyle='-')
ax2.set_ylabel('Porcentaje (%)', color='salmon')
ax2.tick_params(axis='y', labelcolor='salmon')

# Agregar etiquetas de porcentaje en los puntos de la línea (opcional si hay muchos países)
for i, percentage in enumerate(distribucion_porcentual.values):
     ax2.annotate(f'{percentage:.1f}%', (distribucion_porcentual.index[i], percentage),
                  textcoords="offset points", xytext=(5,5), ha='left', color='black')

plt.title('Distribución de Países en el Conjunto de Datos')
plt.tight_layout()
plt.show()

"""Como podemos apreciar en la grafica anterior casi la totalidad de las observaciones conrresponden al Reino unido, razón por la cuál se denotara como la regíon objetivo para la realización del presente trabajo de análisis.

### **4.2. Data preprocessing**
**Objectives**: Perform the data cleaning, data transformation and data reduction steps to avoid data mistmatching, noisy data or data not wrangled
"""

df_baking = df_raw.copy()
df_baking.columns = df_baking.columns.str.lower()
df_baking['invoiceno'] = pd.to_numeric(df_baking['invoiceno'], errors='coerce')
df_baking['stockcode'] = pd.to_numeric(df_baking['stockcode'], errors='coerce')
df_baking['invoicedate'] = pd.to_datetime(df_baking['invoicedate'], errors='coerce')
df_baking['country'] = df_baking['country'].astype('category')
df_baking=df_baking[df_baking['country']=='United Kingdom']
df_baking.reset_index().dropna()
df_baking.info()

df_baking.isnull().sum()

# Eliminar filas donde 'customerid' es nulo (fundamental para la agrupación)
df_baking=df_baking.dropna(subset=['customerid'])
df_baking['description'] = df_baking['description'].str.lower()
df_baking.isnull().sum()

df_baking.info()

"""Para segmentar los clientes debemos igualmente agrupar a los productos que dichos clientes consumen, en este sentido a continuación se muestra cómo se realizó esta agrupación de productos:

#### Creación de un diccionario con las palabras claves para la clasificación de los productos en categorias.
"""

#keywords (dict): A dictionary of categories and their keywords.

keywords = {
        "home_decor": [
            "candle holder", "lantern", "picture frame", "doormat", "mirror",
            "wall art", "clock", "vase", "ornament", "wreath", "hanging decor",
            "heart", "holder", "bell", "decoration", "plant", "sign",
            "coat hanger", "wall decor",
        ],
        "kitchen_and_dining": [
            "teaspoon", "cutlery", "plate", "lunch bag", "mug", "tea set",
            "placemat", "coaster", "napkin", "table cloth", "kitchen scale",
            "oven glove", "tea towel", "trivet", "recipe box", "jar", "tin",
            "canister", "bowl", "chopping board", "therometer", "apron",
            "kitchenware", "dining", "coffee mugs", "breakfast cup", "saucer",
            "milk jug",            "beakers", "enamel jug", "enamel bucket",
            "enamel bread bin","tea for one", "butter dish", "measuring jug",
            "jam making set", "cake cases", "cake stand", "recipe book",
            "cookie cutters", "baking mould", "red retrospot cake stand",
            "egg","hanging eggs", "peg"
        ],
        "storage_and_organization": [
            "box", "cabinet", "storage", "basket", "rack", "organiser",
            "drawer", "tidy", "container","nesting boxes", "snack boxes",
            "box", "cabinet",
            "storage", "basket", "rack", "organiser",
            "drawer", "tidy", "container", "tins"
        ],
        "gifts_and_novelties": [
            "sign", "stationery set",
            "wrapping paper", "card", "party supply", "candle", "soap", "sponge",
            "novelty", "gift", "prank", "magnet", "clock",
            "present", "souvenir", "i love london", "cards"
        ],
        "fashion_accessories": [
            "bag", "purse", "jewellery", "umbrella", "luggage tag",
            "hair accessory", "belt", "scarf", "accessory","bracelet"
        ],
        "toys_and_children's_items": [
            "doll", "toy", "playhouse", "apron", "lunch box",
            "sticker", "game", "child", "kids", "wooden blocks", "drawing slate",
            "puppet", "blocks", " dolly girl beaker","helicopter","baseball",
            "football"
        ],
        "christmas_seasonal_decor": [
            "christmas", "xmas", "advent calendar", "bauble", "tinsel",
            "wreath", "seasonal", "holiday", "new year", "easter",
            "festive"
        ],
        "garden_accessories": [
            "garden", "plant", "herb", "seed", "watering can", "potting",
            "outdoor", "gardening"
        ],
        "decorative_collectibles": [
            "babushka nesting boxes", "building block word", "inflatable globe",
            "wooden frame", "fridge magnets diner", "fridge magnets enfants",
            "photo cube", "antique glass pot","wrap"
        ],
        "party_and_celebration": [
            "paper plates", "cake cases", "party supply", "paper chain",
            "party cones", "paper napkins", "paper cups", "balloons",
            "party candles","party invites", "confetti", "birthday party"
        ],
        "textiles_and_warmth": [
            "hand warmer", "tea towels", "tea cosy", "egg cosy",
            "slipper shoes", "afghan slipper", "hot water bottle","mug cosy",
            "cushion cover", "towel", "scarf",
            "quilt", "fabric", "textile"
        ],
        "games_and_amusements": [
            "jigsaw puzzles", "dominoes", "skittles", "skipping rope",
            "snakes & ladders", "bingo"
        ],
        "office_and_stationery": [
            "paint set", "postage", "stationery set", "wrapping paper",
            "card", "photo clip", "pencils", "pen", "notebooks", "address book"
        ],
        "decorations_and_ornaments": [
            "led night light", "star decoration", "rose caravan",
            "5 hook hanger magic toadstool", "flying ducks", "fairy tale cottage",
            "lovebird hanging", "angel decoration", "wicker star", "chilli lights",
            "light garland", "nightlight", "table light",
            "lamp", "t-lights", "candles"
        ],
        "personal_care": [
            "sewing kit", "soap", "sponge", "tissues"
        ],
        "tools_and_hardware": [
            "tool set", "spirit level", "scissor", "beach spade"
        ],
        "money_banks": [
            "piggy bank", "money bank"
        ],
        "discount_and_fees": [
            "discount", "postage", "bank charges"
        ]
}

"""### Creación de función de clasificación de los productos en categorias"""

def classify_product(description, keywords):
    """
    Categorizes a product description into predefined categories.

    Args:
        description (str): The product description.
        keywords (dict): A dictionary of categories and their keywords.

    Returns:
        str: The category the product belongs to.
    """

    if not isinstance(description, str):
        return "Other"

    #description = description.lower()

    for prod_category, keywords_list in keywords.items():
        for keyword in keywords_list:
            if re.search(r'\b' + re.escape(keyword) + r'\b', description):
                return prod_category
    return "Other"


def apply_classification_to_dataframe(df_baking):
    """
    Applies product classification to a DataFrame with a 'description' column.

    Args:
        df (pd.DataFrame): The DataFrame containing product data.

    Returns:
        pd.DataFrame: The DataFrame with an added 'prod_category' column.
    """

    df_baking = df_baking.copy()
    df_baking['prod_category'] = df_baking['description'].apply(lambda x: classify_product(x, keywords))
    return df_baking

# 2. Apply the classification
df_classified = apply_classification_to_dataframe(df_baking)
df_classified['prod_category'] = df_classified['prod_category'].astype('category')
# 3. Display the result
print(df_classified.sample(10).to_string())
print(df_classified['prod_category'].value_counts())

"""Cóomo se puede observar se han establecido 20 categorias de productos y se ha creado una columna de "prod_category", la cual se ha aderido al dataframe df_classified. A continuación se muestra una grafica con los valores relativos de las categorias de productos."""

# Calculate value counts and percentages
category_counts = df_classified['prod_category'].value_counts()
total_count = category_counts.sum()
category_percentages = (category_counts / total_count) * 100

# Sort for better visualization (optional, but often helpful)
category_percentages = category_percentages.sort_values(ascending=True)

# Create the horizontal bar chart
plt.figure(figsize=(12, 10))  # Adjust figure size as needed
category_percentages.plot(kind='barh')
plt.title('Porcentaje de Productos por Categoría')
plt.xlabel('Porcentaje')
plt.ylabel('Categoría de Producto')

# Add percentage labels to the bars
for index, value in enumerate(category_percentages):
    plt.text(value + 1, index, f'{value:.1f}%', va='center')  # Adjust position as needed

plt.tight_layout()
plt.show()

"""Se puede observar en la grafica que estamos ante la presentcia de una tienda que vende productos no perecederos de forma online, productos que tiene que ver con decoración, juguetes, regalos, ese tipo de artículos de dimensiones pequeñas, por lo cual es muy probable que el grupo de clientes que haga uso de los servicios de este tipo de negocio sea un grupo homogeneo y muy compacto.

Categorías principales: Después de "Other", las categorías más frecuentes son Artículos de decoración de hogar representa un 20.1% de los datos, kitchen_and_dining un 16.2%, storage_and_organization 8.3%, fashion_accessories un 6.6%.  Esto sugiere que estos tipos de productos son los más comunes en el conjunto de datos.

Categorías menos frecuentes: Hay varias categorías con una cantidad relativamente baja de entradas, como "money_banks" (0.2%), "tools_and_hardware" (0.1%) y "discount_and_fees" (0.0..%). Esto podría indicar que estos productos son menos comunes o que estas categorías no están bien representadas en el conjunto de datos.

Igualmente se puede observar que la Categoría 'Other' es la categoría dominante abarcando una cantidad significativamente mayor de entradas (25.8%) que cualquier otra categoría. Esto sugiere que una gran proporción de los productos no pudieron ser clasificados dentro de las categorías predefinidas, que
hay una cantidad importante de productos que no se ajustan a las categorías existentes.

Estos datos representan el número de ocurrencias de cada categoría de producto en el conjunto de datos df_classified. En concreto, muestran la distribución de los productos entre las diferentes categorías.
"""

import pandas as pd
import matplotlib.pyplot as plt

# Assuming your DataFrame is named df and has a column 'invoicedate'

# 1. Convert 'invoicedate' to datetime objects (if it's not already)
#df_classified['invoicedate'] = pd.to_datetime(df_classified['invoicedate'])

# --- Choose the type of plot based on what you want to show ---

# A. Time Series Plot (to show trends over time)
# Group by date and count the number of transactions (or sum a value)
daily_transactions = df_classified.groupby(df_classified['invoicedate'].dt.date).size()  # or .sum()

plt.figure(figsize=(12, 6))
daily_transactions.plot(kind='line')
plt.title('Daily Transactions Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Transactions')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# B. Histogram (to show the distribution of transactions over time periods)
# For example, to see how many transactions happened in each month
df_classified['month_year'] = df_classified['invoicedate'].dt.to_period('M') # Group by month
monthly_transactions = df_classified.groupby('month_year').size()

plt.figure(figsize=(12, 6))
monthly_transactions.plot(kind='bar')
plt.title('Monthly Transactions')
plt.xlabel('Month and Year')
plt.ylabel('Number of Transactions')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# C.  You could also do other aggregations and plots, like:
#     - Transactions by day of the week (to see which days are busiest)
#     - Transactions by hour of the day (if you have the time component)

"""### **4.3. Exploratory Data Analysis**
**Objective**: Summarize the main characteristics of the dataset using descriptive statistics and data visualization methods
"""

df = df_classified.copy()
df.info()

# 1. fecha de corte para la división del conjunto de aprendisaje y prueba
fecha_corte = pd.to_datetime('2011-10-31')

# 2. Se Divide el DataFrame original basado en la fecha
df_train = df[df['invoicedate'] < fecha_corte].copy()
df_test = df[df['invoicedate'] >= fecha_corte].copy()

print(f"Tamaño del conjunto de entrenamiento: {len(df_train)}")
print(f"Tamaño del conjunto de prueba: {len(df_test)}")

# 3. Ingeniería de características para train y test (separadamente)
#    Es importante realizar la agregación y la creación de características
#    de forma independiente para evitar el "data leakage".

# --- Para el conjunto de entrenamiento ---
customer_features_train = df_train.groupby('customerid').agg(
    total_transactions=('invoiceno', 'nunique'),
    total_quantity=('quantity', 'sum'),
    total_spent=('unitprice', 'sum'),
    unique_products=('stockcode', 'nunique'),
    first_purchase=('invoicedate', 'min'),
    last_purchase=('invoicedate', 'max')
)

# --- Para el conjunto de prueba ---
customer_features_test = df_test.groupby('customerid').agg(
    total_transactions=('invoiceno', 'nunique'),
    total_quantity=('quantity', 'sum'),
    total_spent=('unitprice', 'sum'),
    unique_products=('stockcode', 'nunique'),
    first_purchase=('invoicedate', 'min'),
    last_purchase=('invoicedate', 'max')
)

# Fecha de referencia para la recencia (un día después de la última fecha en df_train)
last_train_date = df_train['invoicedate'].max()
now_train = last_train_date + pd.Timedelta(days=1)

# Recencia (Train)
recency_df_train = df_train.groupby('customerid')['invoicedate'].max().reset_index()
recency_df_train['Recency'] = (now_train - recency_df_train['invoicedate']).dt.days
recency_df_train = recency_df_train[['customerid', 'Recency']]

# Frecuencia (Train)
frequency_df_train = df_train.groupby('customerid')['invoiceno'].nunique().reset_index()
frequency_df_train.columns = ['customerid', 'Frequency']

# Valor Monetario (Gasto Total) (Train)
monetary_df_train = df_train.groupby('customerid').agg(Monetary=('unitprice', 'sum')).reset_index()

# Valor Promedio de Compra (Train)
avg_monetary_df_train = df_train.groupby('customerid').agg(AvgMonetary=('unitprice', 'mean')).reset_index()

# CLTV (Train - versión simplificada)
avg_customer_lifetime = 365
cltv_df_train = pd.merge(frequency_df_train, monetary_df_train, on='customerid')
cltv_df_train['CLTV'] = cltv_df_train['Monetary'] * cltv_df_train['Frequency'] / (cltv_df_train['Frequency'].max() if cltv_df_train['Frequency'].max() != 0 else 1)
cltv_df_train = cltv_df_train[['customerid', 'CLTV']]

# Gasto por Tipo de Producto (Train)
#def get_product_type(stockcode):
    #if isinstance(stockcode, str):
        #return stockcode[0]
    #return 'Unknown'

#df_train['ProductType'] = df_train['stockcode'].apply(get_product_type)
product_spending_df_train = df_train.groupby(['customerid', 'prod_category'], observed=True)['unitprice'].sum().unstack(fill_value=0)
product_spending_df_train = product_spending_df_train.add_prefix('Spending_')
product_spending_df_train = product_spending_df_train.reset_index()

# Fusionar todos los ratios (Train)
customer_ratios_train = recency_df_train.merge(frequency_df_train, on='customerid', how='inner')
customer_ratios_train = customer_ratios_train.merge(monetary_df_train, on='customerid', how='inner')
customer_ratios_train = customer_ratios_train.merge(avg_monetary_df_train, on='customerid', how='inner')
customer_ratios_train = customer_ratios_train.merge(cltv_df_train, on='customerid', how='inner')
customer_ratios_train = customer_ratios_train.merge(product_spending_df_train, on='customerid', how='left').fillna(0)
customer_features_train = customer_ratios_train.drop('customerid', axis=1)

# **Preprocesamiento (Escalado) del Conjunto de Entrenamiento:**
scaler = StandardScaler()
scaled_features_train = scaler.fit_transform(customer_features_train)

# 2. Obtener la lista de nombres de las columnas
column_names = customer_features_train.columns.tolist()
print(column_names)

# 3. Mostrar información sobre el DataFrame (tipos de datos y número de no nulos)
print(customer_features_train.info())

# 4. Mostrar estadísticas descriptivas de las columnas numéricas
print(customer_features_train.describe())

# 5. Mostrar las primeras N filas
N = 5
print(customer_features_train.head(N))

# 6. Mostrar un número aleatorio de filas
N = 5
print(customer_features_train.sample(N))

"""### Explicación de la discrepancia en el número de entradas:
La diferencia en el número de entradas se debe a que customer_features_train se crea después de fusionar varios DataFrames (recency_df_train, frequency_df_train,
monetary_df_train, avg_monetary_df_train, cltv_df_train, product_spending_df_train) utilizando 'customerid' como clave de unión.  
Cada uno de estos DataFrames tiene un número diferente de filas, donde cada fila representa una transacción o un cálculo relacionado con un cliente.

Al fusionar estos DataFrames con 'customerid', el número resultante de filas encustomer_features_train será igual al número de 'customerid' únicos que están presentes en todos los DataFrames que se fusionan (o en algunos de ellos,dependiendo del tipo de unión utilizado).

En otras palabras, customer_features_train tiene como filas a los clientes únicos. Por lo tanto, el número de filas en customer_features_train (3614) representa el número de clientes únicos en el conjunto de datos, no el número total de transacciones.  Los 284479 y 77399 representan el número de filas en los conjuntos de entrenamiento y prueba, respectivamente, que muy probablemente representan transacciones individuales.
"""

# 7. Visualización de valores atípicos usando diagramas de caja
# Identificar columnas numéricas para el análisis de valores atípicos
numeric_columns = customer_features_train.select_dtypes(include=['number']).columns
plt.figure(figsize=(20, 10))
customer_features_train[numeric_columns].boxplot()
plt.title('Diagrama de Caja de Columnas Numéricas para Identificar Outliers')
plt.xticks(rotation=45)
plt.show()

# 8. Visualización de la distribución de datos y valores atípicos usando histogramas y diagramas de caja
for col in numeric_columns:
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    sns.histplot(customer_features_train[col], kde=True)
    plt.title(f'Histograma de {col}')
    plt.subplot(1, 2, 2)
    sns.boxplot(y=customer_features_train[col])
    plt.title(f'Diagrama de Caja de {col}')
    plt.show()

def cap_outliers(series, lower_percentile=0.05, upper_percentile=0.90):
    """
    Realiza el capping de valores atípicos en una serie (columna de un DataFrame).

    Args:
        series (pd.Series): La serie de datos donde se van a tratar los outliers.
        lower_percentile (float, opcional): El percentil inferior para el capping. Por defecto es 0.05 (5%).
        upper_percentile (float, opcional): El percentil superior para el capping. Por defecto es 0.95 (95%).

    Returns:
        pd.Series: La serie con los valores atípicos tratados mediante capping.
    """
    # 1. Calcular los límites inferior y superior usando percentiles
    lower_bound = series.quantile(lower_percentile)
    upper_bound = series.quantile(upper_percentile)

    # 2. Aplicar el capping:
    #    - Cualquier valor por debajo del límite inferior se reemplaza por el límite inferior.
    #    - Cualquier valor por encima del límite superior se reemplaza por el límite superior.
    #    - Los valores que están entre lower_bound y upper_bound no se modifican.
    capped_series = series.clip(lower=lower_bound, upper=upper_bound)
    return capped_series

# Aplicar el capping a todas las columnas numéricas de nuestro DataFrame de ratios (train y test)
numeric_columns_train = customer_features_train.select_dtypes(include=['number']).columns  # Obtén todas las columnas numéricas del train
#numeric_columns_test = customer_features_test.select_dtypes(include=['number']).columns # Obtén todas las columnas numéricas del test

for col in numeric_columns_train:
    customer_features_train[col] = cap_outliers(customer_features_train[col])

    #if col in customer_features_test.columns:
        #customer_features_test[col] = cap_outliers(customer_features_test[col])

print("Outliers tratados (capping aplicado) a todas las columnas numéricas.")

def cap_outliers(series, lower_percentile=0.05, upper_percentile=0.90):
    lower_bound = series.quantile(lower_percentile)
    upper_bound = series.quantile(upper_percentile)
    capped_series = series.clip(lower=lower_bound, upper=upper_bound)
    return capped_series

# Aplicar el capping a las columnas relevantes de nuestro DataFrame de ratios (train y test)
columns_to_cap = ['Recency', 'Frequency', 'Monetary', 'CLTV'] # Añade otras columnas si es necesario

for col in columns_to_cap:
    if col in customer_ratios_train.columns:
        customer_ratios_train[col] = cap_outliers(customer_ratios_train[col])
    #if col in customer_ratios_test.columns:
        #customer_ratios_test[col] = cap_outliers(customer_ratios_test[col])

print("Outliers tratados (capping aplicado).")

#2. Correlaciones con la Recencia:
# Correlaciones en el Conjunto de Entrenamiento
correlation_train = customer_ratios_train.corr()['Recency'].sort_values(ascending=False)
print("\n--- Correlaciones con la Recencia (Train) ---")
print(correlation_train)

# Mapa de calor de correlaciones (Train)
plt.figure(figsize=(10, 8))
sns.heatmap(customer_ratios_train.corr(), annot=False, cmap='coolwarm')
plt.title('Mapa de Calor de Correlaciones (Train)')
plt.tight_layout()
plt.show()

"""### **4. Machine learning**
**Objective**: Create a model that learns from data to make predictions and generalize to unseen data, and thus perform tasks without explicit instructions

## Cálculo del número óptimo de Clusters

1. Método del Codo (Elbow Method):
"""

inertia_train = []
k_range = range(1, 11)  # Prueba un rango de valores de k

for k in k_range:
    kmeans_train_elbow = KMeans(n_clusters=k, init='k-means++', n_init='auto', random_state=2025)
    kmeans_train_elbow.fit(scaled_features_train)
    inertia_train.append(kmeans_train_elbow.inertia_)

# Graficar la inercia vs. el número de clusters para el conjunto de entrenamiento
plt.figure(figsize=(10, 6))
plt.plot(k_range, inertia_train, marker='o', linestyle='-')
plt.title('Método del Codo para determinar el número óptimo de Clusters (Conjunto de Entrenamiento)')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Inercia (Within-Cluster Sum of Squares)')
plt.xticks(k_range)
plt.grid(True)
plt.tight_layout()
plt.show()

"""## 2. Análisis del Coeficiente de Silueta (Silhouette Analysis):"""

silhouette_scores_train = []
k_range = range(2, 11)  # El coeficiente de silueta requiere al menos 2 clusters

for k in k_range:
    kmeans_train_silhouette = KMeans(n_clusters=k, init='k-means++', n_init='auto', random_state=2025)
    cluster_labels_train_silhouette = kmeans_train_silhouette.fit_predict(scaled_features_train)
    silhouette_avg_train = silhouette_score(scaled_features_train, cluster_labels_train_silhouette)
    silhouette_scores_train.append(silhouette_avg_train)

# Graficar el coeficiente de silueta promedio vs. el número de clusters para el conjunto de entrenamiento
plt.figure(figsize=(10, 6))
plt.plot(k_range, silhouette_scores_train, marker='o', linestyle='-')
plt.title('Análisis del Coeficiente de Silueta para determinar el número óptimo de Clusters (Conjunto de Entrenamiento)')
plt.xlabel('Número de Clusters (k)')
plt.ylabel('Coeficiente de Silueta Promedio')
plt.xticks(k_range)
plt.grid(True)
plt.tight_layout()
plt.show()

# Imprimir el valor óptimo de k según el coeficiente de silueta
optimal_k_silhouette_train = k_range[np.argmax(silhouette_scores_train)]
print(f"El número óptimo de clusters según el coeficiente de silueta (Train) es: {optimal_k_silhouette_train}")

"""# **3. Clustering (K-means) en el Conjunto de Entrenamiento:**"""

n_clusters = 2 # Define el número de clusters (ajustar con métodos como el codo en el train set)
kmeans_train = KMeans(n_clusters=n_clusters,  init='k-means++', n_init='auto', random_state=2025)
cluster_labels_train = kmeans_train.fit_predict(scaled_features_train)
customer_ratios_train['cluster'] = cluster_labels_train

print("\n--- Perfil de los Clusters en el Conjunto de Entrenamiento (Medias de los Ratios): ---")
print(customer_ratios_train.groupby('cluster').mean().round(4))
cluster_counts_train = customer_ratios_train['cluster'].value_counts()
print("\n--- Número de Customers por Cluster en el Conjunto de Entrenamiento ---")
print(cluster_counts_train)

print("\n--- Métricas de Evaluación del Clustering en el Conjunto de Entrenamiento ---")
silhouette_avg_train = silhouette_score(scaled_features_train, cluster_labels_train)
db_index_train = davies_bouldin_score(scaled_features_train, cluster_labels_train)
ch_index_train = calinski_harabasz_score(scaled_features_train, cluster_labels_train)
print(f"Coeficiente de Silueta Promedio (Train): {silhouette_avg_train:.4f}")
print(f"Índice de Davies-Bouldin (Train): {db_index_train:.4f}")
print(f"Índice de Calinski-Harabasz (Train): {ch_index_train:.4f}")

"""### Métricas de Evaluación del Clustering en el Conjunto de Entrenamiento

Breve interpretación de las métricas de evaluación de clustering que has proporcionado:

#### Coeficiente de Silueta Promedio (Train): 0.9423

El coeficiente de silueta mide qué tan similar es un objeto a su propio clúster en comparación con otros clústeres. Varía de -1 a 1.

Un valor cercano a 1 indica que los objetos están bien agrupados dentro de sus clústeres y están lejos de otros clústeres.

En este caso, un coeficiente de silueta de 0.9423 es excelente, lo que sugiere que los clústeres están muy bien definidos y separados.

#### Índice de Davies-Bouldin (Train): 1.0245

El índice de Davies-Bouldin mide la similitud promedio entre cada clúster y su clúster más similar. Cuanto menor sea el valor, mejor será la agrupación.

Un valor más bajo indica que los clústeres están bien separados y son compactos.

Un índice de 1.0245 es relativamente bajo, lo que indica una buena agrupación.

#### Índice de Calinski-Harabasz (Train): 956.7238

El índice de Calinski-Harabasz mide la relación entre la dispersión entre clústeres y la dispersión dentro del clúster. Un valor más alto indica una mejor agrupación.

Un valor más alto sugiere que los clústeres están bien separados y los objetos dentro de los clústeres son similares entre sí.

El resultado de este modelo es un índice de 956.7238, que es bastante alto, lo que sugiere una buena agrupación.

**En resumen, según estas métricas, el resultado de aplicar el algoritmo K-means para clustering en el conjunto de entrenamiento parece ser bastante bueno. Los clústeres están bien separados, los objetos dentro de los clústeres son similares y la estructura del clúster es clara.**
"""

# **4. Cálculo de los Ratios de Caracterización para el Conjunto de Prueba:**

# Fecha de referencia para la recencia (un día después de la última fecha en df_test)
last_test_date = df_test['invoicedate'].max()
now_test = last_test_date + pd.Timedelta(days=1)

# Recencia (Test)
recency_df_test = df_test.groupby('customerid')['invoicedate'].max().reset_index()
recency_df_test['Recency'] = (now_test - recency_df_test['invoicedate']).dt.days
recency_df_test = recency_df_test[['customerid', 'Recency']]

# Frecuencia (Test)
frequency_df_test = df_test.groupby('customerid')['invoiceno'].nunique().reset_index()
frequency_df_test.columns = ['customerid', 'Frequency']

# Valor Monetario (Gasto Total) (Test)
monetary_df_test = df_test.groupby('customerid').agg(Monetary=('unitprice', 'sum')).reset_index()

# Valor Promedio de Compra (Test)
avg_monetary_df_test = df_test.groupby('customerid').agg(AvgMonetary=('unitprice', 'mean')).reset_index()

# CLTV (Test - usando la misma lógica simplificada)
cltv_df_test = pd.merge(frequency_df_test, monetary_df_test, on='customerid')
cltv_df_test['CLTV'] = cltv_df_test['Monetary'] * cltv_df_test['Frequency'] / (frequency_df_train['Frequency'].max() if frequency_df_train['Frequency'].max() != 0 else 1) # Usar el max de train para consistencia
cltv_df_test = cltv_df_test[['customerid', 'CLTV']]

# Gasto por Tipo de Producto (Test)
#df_test['ProductType'] = df_test['stockcode'].apply(get_product_type)
product_spending_df_test = df_test.groupby(['customerid', 'prod_category'], observed=True)['unitprice'].sum().unstack(fill_value=0)
product_spending_df_test = product_spending_df_test.add_prefix('Spending_')
product_spending_df_test = product_spending_df_test.reset_index()

# Fusionar todos los ratios (Test)
customer_ratios_test = recency_df_test.merge(frequency_df_test, on='customerid', how='inner')
customer_ratios_test = customer_ratios_test.merge(monetary_df_test, on='customerid', how='inner')
customer_ratios_test = customer_ratios_test.merge(avg_monetary_df_test, on='customerid', how='inner')
customer_ratios_test = customer_ratios_test.merge(cltv_df_test, on='customerid', how='inner')
customer_ratios_test = customer_ratios_test.merge(product_spending_df_test, on='customerid', how='left').fillna(0)
customer_features_test = customer_ratios_test.drop('customerid', axis=1)

# **5. Preprocesamiento (Escalado) del Conjunto de Prueba USANDO EL SCALER DEL TRAIN:**
#scaler = StandardScaler()
scaled_features_test = scaler.transform(customer_features_test)# Usar el mismo scaler ajustado en el train
#print(f"Número de filas en scaled_features_test después de transform: {len(scaled_features_test)}")
# **6. Asignar Clusters al Conjunto de Prueba USANDO EL MODELO ENTRENADO EN EL TRAIN:**
#cluster_labels_test = kmeans_train.predict(scaled_features_test)
cluster_labels_test = kmeans_train.predict(scaled_features_test)

customer_ratios_test['cluster'] = cluster_labels_test

#print(f"Longitud de cluster_labels_test después de predict: {len(cluster_labels_test)}")
#print(f"Longitud de customer_ratios_test: {len(customer_ratios_test)}")
#print(f"Longitud de cluster_labels_test: {len(cluster_labels_test)}")
#print(f"Número de filas en customer_features_test: {len(customer_features_test)}")
#print(f"Número de filas en scaled_features_test: {len(scaled_features_test)}")

#customer_ratios_test['cluster'] = cluster_labels_test

"""## Evaluación de las métricas de clustering en el conjunto de prueba"""

if len(scaled_features_test) > 0 and len(cluster_labels_test) == len(scaled_features_test):
    silhouette_avg_test = silhouette_score(scaled_features_test, cluster_labels_test)
    db_index_test = davies_bouldin_score(scaled_features_test, cluster_labels_test)
    ch_index_test = calinski_harabasz_score(scaled_features_test, cluster_labels_test)

    print("\n--- Métricas de Evaluación de la Asignación de Clusters en el Conjunto de Prueba ---")
    print(f"Coeficiente de Silueta Promedio (Test): {silhouette_avg_test:.4f}")
    print(f"Índice de Davies-Bouldin (Test): {db_index_test:.4f}")
    print(f"Índice de Calinski-Harabasz (Test): {ch_index_test:.4f}")
else:
    print("\n--- No se pueden calcular las métricas de evaluación en el Conjunto de Prueba ---")
    print("Asegúrate de que 'scaled_features_test' y 'cluster_labels_test' tengan la misma longitud y no estén vacíos.")

"""### Análisis de los Resultados de la Asignación de Clusters en el Conjunto de Prueba

#### Coeficiente de Silueta Promedio (Test): 0.9645

- El resultado de esta métrica es un valor excelente, incluso mejor que el obtenido en el conjunto de entrenamiento. Indica que los objetos en el conjunto de prueba están muy bien agrupados dentro de sus clusters, mostrando una fuerte separación entre los clusters.

#### Índice de Davies-Bouldin (Test): 0.6759

- El resultado obtenido en esta métrica es considerablemente menor que el obtenido en el conjunto de entrenamiento, lo cual es una mejora significativa. Un valor más bajo del Índice de Davies-Bouldin indica que los clusters están más cohesionados y mejor separados entre sí en el conjunto de prueba.

#### Índice de Calinski-Harabasz (Test): 1631.8700

- El resultado obtenido en esta métrica sustancialmente más alto que el del conjunto de entrenamiento, lo que sugiere que la agrupación en el conjunto de prueba es aún más distinta. Un Calinski-Harabasz más alto refleja una mejor definición de los clusters, con mayor dispersión entre clusters y menor dispersión dentro de ellos.

**En conclusión, las métricas del conjunto de prueba indican un rendimiento de clustering superior al del conjunto de entrenamiento. Esto sugiere que el modelo de clustering se generaliza muy bien a datos no vistos, con clusters bien definidos, compactos y claramente separados. La mejora en todas las métricas clave en el conjunto de prueba es una fuerte indicación de la robustez y eficacia del modelo de clustering.**

## Perfil de los Clusters en el Conjunto de Prueba:
"""

print("\n--- Perfil de los Clusters en el Conjunto de Prueba (Medias de los Ratios): ---")
print(customer_ratios_test.groupby('cluster').mean())
cluster_counts = customer_ratios_test['cluster'].value_counts()
print("\n--- Número de Customers por Cluster en el Conjunto de Prueba ---")
print(cluster_counts)

# Asegúrate de que 'customer_ratios_test' tenga la columna 'cluster'

# 1. Scatter plot de Recencia vs. Frecuencia
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Recency', y='Frequency', hue='cluster', data=customer_ratios_test, palette='viridis')
plt.title('Clusters del Conjunto de Prueba (Recencia vs. Frecuencia)')
plt.xlabel('Recencia (días)')
plt.ylabel('Frecuencia de Compra')
plt.grid(True)
plt.tight_layout()
plt.show()

# 2. Scatter plot de Recencia vs. Valor Monetario
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Recency', y='Monetary', hue='cluster', data=customer_ratios_test, palette='viridis')
plt.title('Clusters del Conjunto de Prueba (Recencia vs. Valor Monetario)')
plt.xlabel('Recencia (días)')
plt.ylabel('Valor Monetario Total')
plt.grid(True)
plt.tight_layout()
plt.show()

# 3. Scatter plot de Frecuencia vs. Valor Monetario
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Frequency', y='Monetary', hue='cluster', data=customer_ratios_test, palette='viridis')
plt.title('Clusters del Conjunto de Prueba (Frecuencia vs. Valor Monetario)')
plt.xlabel('Frecuencia de Compra')
plt.ylabel('Valor Monetario Total')
plt.grid(True)
plt.tight_layout()
plt.show()

import os
import pickle
#from google.colab import drive  # Importar para la integración con Google Drive

# Montar Google Drive si no está ya montado
#if '/content/drive' not in os.listdir('/content'):
    #drive.mount('/content/drive')

# Definir la carpeta en Google Drive donde guardar el modelo
#drive_path = '/content/drive/MyDrive/Colab Notebooks/saved_models/'  # Ajusta la ruta a tu preferencia
#file_name = 'clustering_model.pkl'
#full_path = os.path.join(drive_path, file_name)

# Asegurarse de que la carpeta de destino exista en Google Drive
#os.makedirs(drive_path, exist_ok=True)

# Paso 5: Guardar el modelo de clustering
#print("\nStep 5: Guardar el modelo de clustering en Google Drive")

# Asegúrate de tener el objeto de tu modelo de clustering en una variable
# llamada, por ejemplo, 'kmeans_model'.
#  Asegúrate de que este modelo ESTÉ AJUSTADO (entrenado).
#  Ejemplo:
# from sklearn.cluster import KMeans
# kmeans_model = KMeans(n_clusters=5, random_state=42)
# kmeans_model.fit(X_train) #  <--  ¡¡¡AJUSTA EL MODELO CON TUS DATOS DE ENTRENAMIENTO!!!

model_info = {
    'model': kmeans_train,  # Guardar el objeto del modelo K-Means AJUSTADO
    'model_name': 'K-Means',
    'model_type': 'Clustering',
    'predictors': ['Recency', 'Frequency', 'Monetary', 'CLTV', 'Spending_Other',
                   'Spending_christmas_seasonal_decor', 'Spending_decorations_and_ornaments',
                   'Spending_decorative_collectibles', 'Spending_discount_and_fees',
                   'Spending_fashion_accessories', 'Spending_games_and_amusements'],
    'n_clusters': kmeans_train.n_clusters,  # Usar el atributo del modelo
    #'training_data_shape': X_train.shape,  # Guarda la forma de los datos de entrenamiento
    'metrics_train': {  # Agrupar métricas de train y test
        'silhouette_score': silhouette_avg_train,
        'davies_bouldin_score': db_index_train,
        'calinski_harabasz_score': ch_index_train
    },
    'metrics_test': {
        'silhouette_score': silhouette_avg_test,
        'davies_bouldin_score': db_index_test,
        'calinski_harabasz_score': ch_index_test
    },
    '#Author': 'Kleineer Tabata'
}

# Guardar el archivo del modelo usando pickle en Google Drive
# with open(full_path, 'wb') as file:
    #pickle.dump(model_info, file)

# print(f"Modelo de clustering guardado en Google Drive: {full_path}")

#"""## Conclusiones y recomendaciones

### Conclusiones sobre la Aplicación del Algoritmo K-means

#A partir de las métricas de evaluación del desempeño del algoritmo de clustering (K-means), podemos llegar a las siguientes conclusiones sobre la aplicación del algoritmo K-means al conjunto de datos:



#1.   El algoritmo K-means logró identificar una estructura de clústeres en los datos, como lo indican los valores de las métricas.La calidad del clustering es relativamente buena, sugiriendo que los clientes pueden segmentarse en grupos distintos según sus características.

#2.   Análizando mas detalladamente las métricas podemos concluir:

#- **Coeficiente de Silueta**: El coeficiente de silueta mide qué tan similar es un punto a su propio clúster en comparación con otros clústeres. Un valor de 0.9423 en el conjunto de entrenamiento indica que los clústeres están bien separados y los puntos dentro de cada clúster son muy similares entre sí.El valor de 0.9645 en el conjunto de prueba es incluso mejor, lo que sugiere que el clustering se generaliza bien a datos no vistos.

#- **Índice de Davies-Bouldin**: El índice de Davies-Bouldin mide la similitud promedio entre cada clúster y su clúster más similar. Cuanto menor sea el valor, mejor será la agrupación.Un valor de 1.0245 en el conjunto de entrenamiento indica que hay cierta superposición entre los clústeres.El valor de 0.6759 en el conjunto de prueba es considerablemente mejor, lo que confirma que los clústeres están bien separados.

#- **Índice de Calinski-Harabasz**:El índice de Calinski-Harabasz mide la relación entre la dispersión entre clústeres y la dispersión dentro del clúster. Un valor más alto indica una mejor agrupación.Un valor de 956.7238 en el conjunto de entrenamiento sugiere que los clústeres son relativamente distintos. El valor de 1631.8698 en el conjunto de prueba es sustancialmente más alto, lo que indica una mejor separación de los clústeres en datos no vistos.

#3.   Los ratios calculados son importantes para homogeneizar y entender el comportamiento del cliente y puedan ser usados como variables de entrada en la aplicación del algoritmo de K-means para la segmentación de clientes:

#- **Recencia**: Mide el tiempo desde la última compra del cliente. Un valor más bajo indica un cliente más activo y comprometido.

#- **Frecuencia**: Indica el número de compras que ha realizado un cliente en un periodo de tiempo. Un valor más alto sugiere mayor lealtad.

#- **Valor Monetario (Gasto Total)**: Es la cantidad total de dinero que un cliente ha gastado en la empresa. Un valor más alto indica clientes más valiosos.

#- **Valor Promedio de Compra**: Se calcula dividiendo el gasto total entre el número de compras. Ayuda a entender el gasto típico por transacción.

#- **CLTV (Customer Lifetime Value)**: Predice el valor total que un cliente aportará a la empresa a lo largo de su relación. Es útil para enfocar esfuerzos de retención en clientes de alto valor.

#- **Gasto por Tipo de Producto**: Indica cuánto gasta un cliente en diferentes categorías de productos. Permite identificar las preferencias de los clientes y personalizar ofertas.

#4.   La clara estructura de clústeres identificada en el desarrollo del modelo implementando el algoritmo K-means puede ser útil para segmentar a los clientes en diferentes grupos con características y comportamientos similares. Esta segmentación puede soportar estrategias de marketing específicas, como adaptar las ofertas y los mensajes a las necesidades e intereses únicos de cada segmento. Al comprender las diferencias entre los grupos de clientes, las empresas puede optimizar sus esfuerzos de marketing y mejorar la satisfacción de sus clientes.



### Recomendaciones



#1.   Generar una mejor plataforma para la captura de datos de los clientes y poder generar mejores imputs al modelo. Adicionalmente y partiendo de los datos analizados se puede implementar estrategias que permitan incrementar el número único de clientes.

#2.   Considerar que las métricas de evaluación solo proporcionan una indicación general de la calidad del clustering. Es importante complementar estos resultados con un análisis más profundo de las características de cada clúster y su relevancia para el negocio.

#3. Considerar Ratios que se ajusten de forma clara e inequivoca a los clientes y que permitan generar de manera fiable una estructura que permita evaluar de forma equilibrada a cada cliente. K-means es sensible a la escala de las variables. Si las variables tienen escalas muy diferentes, es importante estandarizarlas antes de aplicar el algoritmo.

#4. La elección del número óptimo de clústeres (k) es crucial para el rendimiento de K-means. Si no se selecciona cuidadosamente, el algoritmo puede producir resultados subóptimos, por ende, se debe siempre considerar la metódología de elección del número optimo de clusters que mas se adapta a la realidad.

## Referencias